{"train_batch_size": 16, "train_micro_batch_size_per_gpu": 2, "gradient_accumulation_steps": 8, "optimizer": {"type": "Adam", "params": {"lr": 0.00016, "betas": [0.9, 0.95], "eps": 1e-08}}, "fp16": {"enabled": true, "loss_scale": 0, "loss_scale_window": 1000, "hysteresis": 2, "min_loss_scale": 1}, "zero_optimization": {"stage": 0, "allgather_partitions": true, "reduce_scatter": true, "allgather_bucket_size": 500000000, "overlap_comm": false, "reduce_bucket_size": 500000000, "contiguous_gradients": false}, "steps_per_print": 1, "wall_clock_breakdown": true, "precision": "fp16", "num_layers": 32, "hidden_size": 2560, "num_attention_heads": 32, "seq_length": 2048, "max_position_embeddings": 2048, "pos_emb": "rotary", "no_weight_tying": true, "attention_config": ["global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global"], "sparsity_config": {}, "scaled_upper_triang_masked_softmax_fusion": true, "bias_gelu_fusion": true, "init_method": "small_init", "output_layer_init_method": "wang_init", "output_layer_parallelism": "column", "lr_decay_style": "cosine", "lr_decay_iters": 250000, "min_lr": 1.6e-05, "optimizer_type": "Adam", "zero_stage": 0, "zero_reduce_scatter": true, "zero_contiguous_gradients": false, "zero_reduce_bucket_size": 500000000, "zero_allgather_bucket_size": 500000000, "lr": 0.00016, "tokenizer_type": "HFTokenizer", "train_data_paths": ["/root/ft_data/OIG/tokenized/unified_ni/_text_document", "/root/ft_data/OIG/tokenized/unified_flan/_text_document", "/root/ft_data/OIG/tokenized/unified_chip2/_text_document", "/root/ft_data/OIG/tokenized/unified_rallio_safety_and_prosocial/_text_document", "/root/ft_data/OIG/tokenized/unified_soda_dialog/_text_document", "/root/ft_data/OIG/tokenized/unified_unifiedskg_instructions/_text_document", "/root/ft_data/OIG/tokenized/unified_merged_code_xp3/_text_document", "/root/ft_data/OIG/tokenized/unified_oscar_en_sample_dialog/_text_document", "/root/ft_data/OIG/tokenized/unified_ul2_plus_oscar_en_sample_dialog/_text_document", "/root/ft_data/OIG/tokenized/unified_multi_news/_text_document", "/root/ft_data/OIG/tokenized/unified_openai_summarize_tldr/_text_document", "/root/ft_data/OIG/tokenized/unified_squad_v2/_text_document", "/root/ft_data/OIG/tokenized/unified_nq/_text_document", "/root/ft_data/OIG/tokenized/unified_poetry_instructions/_text_document", "/root/ft_data/OIG/tokenized/unified_sqlv2/_text_document", "/root/ft_data/OIG/tokenized/unified_unnatural_instructions/_text_document", "/root/ft_data/OIG/tokenized/unified_conv_finqa/_text_document", "/root/ft_data/OIG/tokenized/unified_essays/_text_document", "/root/ft_data/OIG/tokenized/unified_plot_screenplay_books_dialog/_text_document", "/root/ft_data/OIG/tokenized/unified_grade_school_math_instructions/_text_document", "/root/ft_data/OIG/tokenized/unified_mathqa_flanv2_kojma_cot/_text_document", "/root/ft_data/OIG/tokenized/unified_joke_explanations/_text_document", "/root/ft_data/OIG/tokenized/unified_cuad/_text_document", "/root/ft_data/OIG/tokenized/unified_abstract_infill/_text_document", "/root/ft_data/OIG/tokenized/unified_image_prompts_instructions/_text_document"], "test_data_paths": ["/root/ft_data/OIG/tokenized/unified_image_prompts_instructions/_text_document"], "valid_data_paths": ["/root/ft_data/OIG/tokenized/unified_image_prompts_instructions/_text_document"], "train_data_weights": [0.7, 0.2, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.05, 0.05, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.1, 0.01], "valid_data_weights": [1.0], "test_data_weights": [1.0], "data_impl": "mmap", "save": "/root/fm/models/ckpts/model_checkpoints", "config_files": {"base.yml": "{\n   \"finetune\": True,\n   # parallelism settings ( you will want to change these based on your cluster setup, ideally scheduling pipeline stages\n   # across the node boundaries )\n   \"pipe-parallel-size\": 4,\n   \"model-parallel-size\": 2, # one copy of the model per node\n   # model settings\n   \"num-layers\": 32,\n   \"hidden-size\": 2560,\n   \"num-attention-heads\": 32,\n   \"seq-length\": 2048,\n   \"max-position-embeddings\": 2048,\n   \"norm\": \"layernorm\",\n   \"pos-emb\": \"rotary\",\n   \"no-weight-tying\": true,\n   \"gpt_j_residual\": false,\n   \"output_layer_parallelism\": \"column\",\n\n   # these should provide some speedup but takes a while to build, set to true if desired\n   \"scaled-upper-triang-masked-softmax-fusion\": true,\n   \"bias-gelu-fusion\": true,\n\n   # init methods\n   \"init_method\": \"small_init\",\n   \"output_layer_init_method\": \"wang_init\",\n \n   # optimizer settings\n   \"optimizer\": {\n     \"type\": \"Adam\",\n     \"params\": {\n       \"lr\": 0.00016,\n       \"betas\": [0.9, 0.95],\n       \"eps\": 1.0e-8,\n     }\n   },\n   \"min_lr\": 0.000016,\n   # for all zero_optimization options, see https://www.deepspeed.ai/docs/config-json/#zero-optimizations-for-fp16-training\n   #\"zero_optimization\": {\n   # \"stage\": 1,\n   # \"allgather_partitions\": True,\n   # \"allgather_bucket_size\": 500000000,\n   # \"overlap_comm\": True,\n   # \"reduce_scatter\": True,\n   # \"reduce_bucket_size\": 500000000,\n   # \"contiguous_gradients\": True,\n  #},\n\n   # batch / data settings\n   \"train_batch_size\": 16, # across 1024 nodes... fingers crossed\n  #  \"train_micro_batch_size_per_gpu\": 4,\n   \"gradient_accumulation_steps\": 8,\n   \"data-impl\": \"mmap\",\n   \"split\": \"949,50,1\",\n\n   # activation checkpointing\n   \"checkpoint-activations\": true,\n   \"checkpoint-num-layers\": 1,\n   \"partition-activations\": true,\n   \"synchronize-each-layer\": true,\n\n   # regularization\n   \"gradient_clipping\": 1.0,\n   \"weight-decay\": 0.1,\n   \"hidden-dropout\": 0.0,\n   \"attention-dropout\": 0.0,\n\n   # precision settings\n   \"fp16\": {\n     \"enabled\": true,\n    #  \"type\": \"bfloat16\", # set bf16 as precision\n     \"loss_scale\": 0,\n     \"loss_scale_window\": 1000,\n     \"hysteresis\": 2,\n     \"min_loss_scale\": 1\n   },\n\n  #  \"fp32_allreduce\": True, # without a patch to torch, bf16 models have to do the allreduce in fp32\n   # misc. training settings\n   \"train-iters\": 250000,\n   \"lr-decay-iters\": 250000,\n   \"distributed-backend\": \"nccl\",\n   \"lr-decay-style\": \"cosine\",\n   \"warmup\": 0.01,\n   \"checkpoint-factor\": 5000,\n   \"eval-interval\": 5000,\n   \"eval-iters\": 10,\n\n   # logging\n   \"log-interval\": 1,\n   \"steps_per_print\": 1,\n   \"keep-last-n-checkpoints\": 1000,\n   \"wall_clock_breakdown\": true,\n}\n", "oig.yml": "# Suggested data paths when using GPT-NeoX locally\n{\n  # \"data-path\": \"data/enwik8/enwik8_text_document\",\n\n  # or for weighted datasets:\n  \"train-data-paths\":\n    [\n      \"/root/ft_data/OIG/tokenized/unified_ni/_text_document\",\n      # \"/root/ft_data/OIG/tokenized/unified_p3/_text_document\",\n      \"/root/ft_data/OIG/tokenized/unified_flan/_text_document\",\n      \"/root/ft_data/OIG/tokenized/unified_chip2/_text_document\",\n      \"/root/ft_data/OIG/tokenized/unified_rallio_safety_and_prosocial/_text_document\",\n      \"/root/ft_data/OIG/tokenized/unified_soda_dialog/_text_document\",\n      \"/root/ft_data/OIG/tokenized/unified_unifiedskg_instructions/_text_document\",\n      \"/root/ft_data/OIG/tokenized/unified_merged_code_xp3/_text_document\",\n      \"/root/ft_data/OIG/tokenized/unified_oscar_en_sample_dialog/_text_document\",\n      \"/root/ft_data/OIG/tokenized/unified_ul2_plus_oscar_en_sample_dialog/_text_document\",\n      \"/root/ft_data/OIG/tokenized/unified_multi_news/_text_document\",\n      \"/root/ft_data/OIG/tokenized/unified_openai_summarize_tldr/_text_document\",\n      \"/root/ft_data/OIG/tokenized/unified_squad_v2/_text_document\",\n      \"/root/ft_data/OIG/tokenized/unified_nq/_text_document\",\n      \"/root/ft_data/OIG/tokenized/unified_poetry_instructions/_text_document\",\n      \"/root/ft_data/OIG/tokenized/unified_sqlv2/_text_document\",\n      \"/root/ft_data/OIG/tokenized/unified_unnatural_instructions/_text_document\",\n      \"/root/ft_data/OIG/tokenized/unified_conv_finqa/_text_document\",\n      \"/root/ft_data/OIG/tokenized/unified_essays/_text_document\",\n      \"/root/ft_data/OIG/tokenized/unified_plot_screenplay_books_dialog/_text_document\",\n      \"/root/ft_data/OIG/tokenized/unified_grade_school_math_instructions/_text_document\",\n      \"/root/ft_data/OIG/tokenized/unified_mathqa_flanv2_kojma_cot/_text_document\",\n      \"/root/ft_data/OIG/tokenized/unified_joke_explanations/_text_document\",\n      \"/root/ft_data/OIG/tokenized/unified_cuad/_text_document\",\n      \"/root/ft_data/OIG/tokenized/unified_abstract_infill/_text_document\",\n      \"/root/ft_data/OIG/tokenized/unified_image_prompts_instructions/_text_document\",\n    ],\n  \"test-data-paths\": [\"/root/ft_data/OIG/tokenized/unified_image_prompts_instructions/_text_document\"],\n  \"valid-data-paths\": [\"/root/ft_data/OIG/tokenized/unified_image_prompts_instructions/_text_document\"],\n  \"train-data-weights\":\n    [\n      0.7,\n      # 0.5,\n      0.2,\n      0.01,\n      0.1,\n      0.1,\n      0.1,\n      0.1,\n      0.1,\n      0.1,\n      0.05,\n      0.05,\n      0.01,\n      0.01,\n      0.01,\n      0.01,\n      0.01,\n      0.01,\n      0.01,\n      0.01,\n      0.01,\n      0.01,\n      0.01,\n      0.01,\n      0.1,\n      0.01,\n    ],\n  \"test-data-weights\": [1.,],\n  \"valid-data-weights\": [1.,],\n\n  # If weight_by_num_documents is True, Builds dataset weights from a multinomial distribution over groups of data according to the number of documents in each group.\n  # WARNING: setting this to True will override any user provided weights\n  # \"weight_by_num_documents\": false,\n  # \"weighted_sampler_alpha\": 0.3,\n  \"tokenizer_type\": \"HFTokenizer\",\n  \"vocab-file\": \"/root/ft_data/meta/20B_tokenizer.json\",\n  # \"merge-file\": \"/root/fm/config/gpt2-merges.txt\",\n  # \"finetune\": True,\n  \"save\": \"/root/fm/models/ckpts/model_checkpoints\",\n  \"load\": \"/root/fm/models/ckpts/model_global_step100000\",\n  \"checkpoint_validation_with_forward_pass\": False,\n  \"tensorboard-dir\": \"tensorboard\",\n  \"log-dir\": \"logs\",\n  \"use_wandb\": True,\n  \"wandb_host\": \"https://api.wandb.ai\",\n  \"wandb_project\": \"neox\",\n}\n"}, "load": "/root/fm/models/ckpts/model_global_step100000", "checkpoint_factor": 5000, "finetune": true, "batch_size": 2, "train_iters": 250000, "eval_iters": 10, "keep_last_n_checkpoints": 1000, "eval_interval": 5000, "split": "949,50,1", "vocab_file": "/root/ft_data/meta/20B_tokenizer.json", "attention_dropout": 0.0, "hidden_dropout": 0.0, "weight_decay": 0.1, "checkpoint_activations": true, "synchronize_each_layer": true, "partition_activations": true, "gas": 8, "clip_grad": 1.0, "dynamic_loss_scale": true, "pipe_parallel_size": 4, "model_parallel_size": 2, "world_size": 1, "is_pipe_parallel": true, "use_wandb": true, "wandb_group": "frjzsic4_0hrzncx8", "log_dir": "logs", "tensorboard_dir": "tensorboard", "log_interval": 1, "text_gen_type": "unconditional", "local_rank": 0, "rank": 0, "user_script": "train.py", "save_iters": [5000, 10000, 15000, 20000, 25000, 30000, 35000, 40000, 45000, 50000, 55000, 60000, 65000, 70000, 75000, 80000, 85000, 90000, 95000, 100000, 105000, 110000, 115000, 120000, 125000, 130000, 135000, 140000, 145000, 150000, 155000, 160000, 165000, 170000, 175000, 180000, 185000, 190000, 195000, 200000, 205000, 210000, 215000, 220000, 225000, 230000, 235000, 240000, 245000], "global_num_gpus": 8}